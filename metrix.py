def metrix(modelname, validation, predicted):
    confusion = confusion_matrix(validation, predicted)
    weighted_matrix = np.array([np.round(confusion[i]/sum(confusion[i])*100,0) for i in range(len(confusion))])
    f1 = f1_score(validation, predicted, average ='macro')
    fp5 = fbeta_score(validation, predicted, average='macro', beta=0.5)
    f2 = fbeta_score(validation, predicted, average='macro', beta=2)
    precision = precision_score(validation, predicted, average='macro')
    recall = recall_score(validation, predicted, average='macro')
    matthews = matthews_corrcoef(validation, predicted)
    acc = accuracy_score(validation, predicted)
    mse = mean_squared_error(y_val, slr_pred)

    print('\nConfusion Matrix\n'+str(confusion))
    print('\nConfusion Matrix as Percentages\n'+str(weighted_matrix))
    print('\nF1 Score: '+str(f1))
    print('F0.5 Score: '+str(fp5))
    print('F2 Score: '+str(f2))
    print('Precision: '+str(precision))
    print('Recall: '+str(recall))
    print('Matthews Correlation: '+str(matthews))
    print('Accuracy: '+str(acc))
    print('Mean Squared Error: '+str(mse)+'\n')

    df = pd.DataFrame(weighted_matrix, index = ['negligible','moderate','severe'], columns = ['negligible','moderate','severe'])

    fig, ax = plt.subplots(ncols=1, figsize=(7, 4))
    fig.subplots_adjust(wspace=0.1)
    ax = sn.heatmap(df, cmap=plt.cm.Blues, ax=ax, cbar=False, annot=True, vmin=0, vmax=100)
    fig.colorbar(ax.collections[0], ax=ax,location="left", use_gridspec=False, pad=0.2)
    for t in ax.texts: t.set_text(t.get_text() + " %")
    sn.set(font_scale=1.4)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')
    ax.title.set_text('%s' %modelname)
