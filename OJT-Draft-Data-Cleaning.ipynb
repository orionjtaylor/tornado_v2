{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package import cell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged tornadoes+income+density data\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv('/users/Orion/NYU/tornado_v2/Merged-Tornadoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns and set EVENT_ID as index\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0', 'EPISODE_ID', 'EVENT_TYPE', 'WFO', 'SOURCE', 'MAGNITUDE', \n",
    "                          'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', \n",
    "                          'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', \n",
    "                          'BEGIN_AZIMUTH', 'END_AZIMUTH', 'BEGIN_LOCATION', 'END_LOCATION', \n",
    "                          'EPISODE_NARRATIVE', 'DATA_SOURCE', 'BEGIN_YEARMONTH', 'END_YEARMONTH', \n",
    "                          'MONTH_NAME', 'CZ_TIMEZONE', 'YEAR', 'STATE_FIPS', 'CZ_TYPE', 'CZ_FIPS', \n",
    "                          'State FIPS Code', 'Name', 'Geographic area', 'Geographic area.1', \n",
    "                          'Population', 'Housing units', 'STATE', 'CZ_NAME', 'END_TIME'])\n",
    "data = data.set_index('EVENT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns for clarity\n",
    "\n",
    "data = data.rename(columns={'Density per square mile of land area - Population': 'population_density',\n",
    "                            'Density per square mile of land area - Housing units': 'housing_units_density', \n",
    "                            'Area in square miles - Total area': 'total_area', \n",
    "                            'Area in square miles - Land area': 'land_area',\n",
    "                            'BEGIN_TIME': 'begin_time', 'Median Household Income': 'median_income'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert begin and end dates to datetime and get tornado duration\n",
    "\n",
    "data['BEGIN_DATE_TIME'] =  pd.to_datetime(data['BEGIN_DATE_TIME'])\n",
    "data['END_DATE_TIME'] =  pd.to_datetime(data['END_DATE_TIME'])\n",
    "data['duration'] = data['END_DATE_TIME'] - data['BEGIN_DATE_TIME']\n",
    "data['duration'] = data['duration'].dt.seconds/60\n",
    "data = data.drop(columns=['END_DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'BEGIN_DATE_TIME' to datetime; define\n",
    "# function to convert to integer 1 through 365 and\n",
    "# convert using this function; for each day \n",
    "# determine if weekend or weekday\n",
    "\n",
    "def date_to_nth_day(date, format='%Y%m%d'):\n",
    "    new_year_day = datetime.datetime(year=date.year, month=1, day=1)\n",
    "    return (date - new_year_day).days + 1\n",
    "\n",
    "data['day_of_year'] = data['BEGIN_DATE_TIME'].map(date_to_nth_day)\n",
    "data['day_of_week'] = data['BEGIN_DATE_TIME'].dt.weekday\n",
    "data['weekend'] = data['day_of_week'].map({0:0, 1:0, 2:0, 3:0, 4:0, 5:1, 6:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time features cyclically\n",
    "\n",
    "def convert_to_mins(time_in_24_hours):\n",
    "    stringtime = str(time_in_24_hours)\n",
    "\n",
    "    if (len(stringtime) == 4):\n",
    "        hours_as_mins = int(stringtime[0:2])*60\n",
    "        mins = int(stringtime[2:4])\n",
    "        total_mins = hours_as_mins + mins\n",
    "    elif (len(stringtime) == 3):\n",
    "        hours_as_mins = int(stringtime[0])*60\n",
    "        mins = int(stringtime[1:3])\n",
    "        total_mins = hours_as_mins + mins\n",
    "    elif (len(stringtime) < 3):\n",
    "        total_mins = int(stringtime)\n",
    "    else:\n",
    "        print('Bad Data')\n",
    "        assert False\n",
    "    return total_mins\n",
    "\n",
    "data['begin_time'] = data['begin_time'].apply(convert_to_mins)\n",
    "\n",
    "minutes_in_a_day = 24*60\n",
    "data['sin_time'] = np.sin(2*np.pi*data['begin_time']/minutes_in_a_day)\n",
    "data['cos_time'] = np.cos(2*np.pi*data['begin_time']/minutes_in_a_day)\n",
    "\n",
    "days_in_a_year = 365\n",
    "data['sin_date'] = np.sin(2*np.pi*data['day_of_year']/days_in_a_year)\n",
    "data['cos_date'] = np.cos(2*np.pi*data['day_of_year']/days_in_a_year)\n",
    "\n",
    "data = data.drop(columns=['begin_time', 'day_of_year', 'BEGIN_DAY', \n",
    "                          'END_DAY', 'day_of_week', 'BEGIN_DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert all injury- and death-related columns to numeric, sum\n",
    "# and create casualties from sum\n",
    "\n",
    "data['INJURIES_DIRECT'] = pd.to_numeric(data['INJURIES_DIRECT'])\n",
    "data['INJURIES_INDIRECT'] = pd.to_numeric(data['INJURIES_INDIRECT'])\n",
    "data['DEATHS_DIRECT'] = pd.to_numeric(data['DEATHS_DIRECT'])\n",
    "data['DEATHS_INDIRECT'] = pd.to_numeric(data['DEATHS_INDIRECT'])\n",
    "data[\"casualties\"] = (data[\"INJURIES_INDIRECT\"]+data[\"INJURIES_DIRECT\"]+\n",
    "                      data[\"DEATHS_DIRECT\"]+data[\"DEATHS_INDIRECT\"])\n",
    "data = data.drop(columns=[\"INJURIES_INDIRECT\", \"INJURIES_DIRECT\", \n",
    "                          \"DEATHS_INDIRECT\", \"DEATHS_DIRECT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert EF_Scale to binary (1 for 2+, 0 for 0,1)\n",
    "\n",
    "def makeEFBinary(ef):\n",
    "    if ef <= 1:\n",
    "        return 0\n",
    "    elif ef <= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "data = data[data['TOR_F_SCALE'] != 'EFU']\n",
    "data['TOR_F_SCALE'] = data['TOR_F_SCALE'].map(lambda x: int(x.lstrip('EF')))\n",
    "data[\"tornado_intensity\"] = data[\"TOR_F_SCALE\"].map(makeEFBinary)\n",
    "data = data.drop(columns='TOR_F_SCALE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tornado area from length and width\n",
    "\n",
    "data['TOR_LENGTH'] = data['TOR_LENGTH'].map(lambda x: int(x))\n",
    "data['tornado_area'] = data['TOR_LENGTH']*data['TOR_WIDTH']\n",
    "data = data.drop(columns=['TOR_LENGTH', 'TOR_WIDTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean and min of 'BEGIN_RANGE' and 'END_RANGE', \n",
    "# roughly proxying distance from population center\n",
    "\n",
    "data['average_range'] = data.loc[:, ['BEGIN_RANGE','END_RANGE']].mean(axis = 1)\n",
    "data['minimum_range'] = data.loc[:, ['BEGIN_RANGE','END_RANGE']].min(axis = 1)\n",
    "data = data.drop(columns=['BEGIN_RANGE', 'END_RANGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average lat/long\n",
    "\n",
    "data['average_latitude'] = (data['BEGIN_LAT'] + data['END_LAT'])/2\n",
    "data[\"average_longitude\"] = (data['BEGIN_LON'] + data['END_LON'])/2\n",
    "data = data.drop(columns=['BEGIN_LAT', 'END_LAT', 'BEGIN_LON', 'END_LON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent land\n",
    "\n",
    "data['percent_land'] = data['land_area']/data['total_area']\n",
    "data = data.drop(columns=['land_area', 'total_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract multi-vortex references from EVENT_NARRATIVE\n",
    "\n",
    "data['multi_vortex'] = 0\n",
    "data.loc[data.apply(lambda x: 'multi-vortex' in x['EVENT_NARRATIVE'], axis=1), ['multi_vortex']] = 1\n",
    "data.loc[data.apply(lambda x: 'multiple vortex' in x['EVENT_NARRATIVE'], axis=1), ['multi_vortex']] = 1\n",
    "data.loc[data.apply(lambda x: 'multiple vortices' in x['EVENT_NARRATIVE'], axis=1), ['multi_vortex']] = 1\n",
    "data = data.drop(columns=['EVENT_NARRATIVE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize median income, population density, housing units density\n",
    "# and percent land and replace NaNs with average\n",
    "\n",
    "data['median_income'] = data['median_income'].str.replace(\",\", \"\").astype(float)\n",
    "mean_Median_Household_Income = data['median_income'].mean(skipna = True)\n",
    "data['median_income'] = data['median_income'].fillna(mean_Median_Household_Income)\n",
    "\n",
    "data['population_density'] = data['population_density'].astype(float)\n",
    "mean_Pop_Density = data['population_density'].mean(skipna = True)\n",
    "data['population_density'] = data['population_density'].fillna(mean_Pop_Density)\n",
    "\n",
    "data['housing_units_density'] = data['housing_units_density'].astype(float)\n",
    "mean_Housing_Units_Density = data['housing_units_density'].mean(skipna = True)\n",
    "data['housing_units_density'] = data['housing_units_density'].fillna(mean_Housing_Units_Density)\n",
    "\n",
    "data['percent_land'] = data['percent_land'].astype(float)\n",
    "mean_Percent_Land = data['percent_land'].mean(skipna = True)\n",
    "data['percent_land'] = data['percent_land'].fillna(mean_Percent_Land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary equivalent for casualties and\n",
    "# tri-class casualties bins (0, 1-19, 20+)\n",
    "\n",
    "data['binary_casualties'] = np.where(data['casualties']>=1, 1, 0)\n",
    "data['multiclass_casualties'] = 0\n",
    "data.loc[(data['casualties'] > 0) & (data['casualties'] < 20), 'multiclass_casualties'] = 1\n",
    "data.loc[(data['casualties'] > 19), 'multiclass_casualties'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames from classification (binary)\n",
    "# and regression (non-binary)\n",
    "\n",
    "data_binary = data.drop(columns=['casualties', 'multiclass_casualties'])\n",
    "data_multiclass = data.drop(columns=['casualties', 'binary_casualties'])\n",
    "data_regress = data.drop(columns=['binary_casualties', 'multiclass_casualties'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to separate CSVs\n",
    "\n",
    "data_binary.to_csv('tornadoes-binary.csv')\n",
    "data_multiclass.to_csv('tornadoes-multiclass.csv')\n",
    "data_regress.to_csv('tornadoes-nonbinary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
